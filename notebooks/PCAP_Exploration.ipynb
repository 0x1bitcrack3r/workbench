{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# PCAP Exploration\n",
      "An exploration of pcaps using Workbench https://github.com/ClickSecurity/workbench.git\n",
      "## Goals of the Exploration\n",
      "PCAPS can be opaque, so what do we have, how do we organize it, how do we glean some knowledge?\n",
      "\n",
      "**Note:** Obviously there are super great tools that already perform exploration and analysis of pcaps: WireShark, Chop Shop, Scapy, blah, foo, etc.. here we leveraging Bro IDS to generate our starting-point data and then hoping off in various directions from there. The work here should be viewed as complimentary to these other tools and you might consider putting into your toolset. :)\n",
      "\n",
      "### Start up the workbench server...\n",
      "Run the workbench server (from somewhere, for the demo we're just going to start a local one)\n",
      "<pre>\n",
      "> cd workbench/server\n",
      "> python -O workbench.py\n",
      "</pre>\n",
      "\n",
      "#### Okay so when the server starts up, it autoloads any worker plugins in the server/worker directory and dynamically monitors the directory, if a new python file shows up, it's validated as a properly formed plugin and if it passes is added to the list of workers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "np.__version__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "'1.8.0'"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "pd.__version__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "'0.13.0rc1-32-g81053f9'"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plotting defaults\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "plt.rcParams['font.size'] = 12.0\n",
      "plt.rcParams['figure.figsize'] = 18.0, 8.0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Lets start to interact with workbench, please note there is NO specific client to workbench,\n",
      "# Just use the ZeroRPC Python, Node.js, or CLI interfaces.\n",
      "import zerorpc\n",
      "c = zerorpc.Client(passive_heartbeat=True, timeout=300)\n",
      "c.connect(\"tcp://127.0.0.1:4242\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[None]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Want to find out about what's on the other side?\n",
      "print c._zerorpc_name()\n",
      "print c._zerorpc_list()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "workbench\n",
        "['get_datastore_url', 'search', 'work_request', 'worker_info', 'store_sample', 'get_sample', 'batch_work_request', 'index_sample', 'have_sample', 'stream_sample', 'index_worker_output']\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Okay so we see that we're connected to workbench and the calls we can make\n",
      "# The 'worker_info' call look interesting...\n",
      "c.worker_info()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "{'json_meta': 'This worker computes a meta-data for log files. Output keys: [md5, type_tag, file_type, mime_type, encoding, file_size, filename, import_time]',\n",
        " 'log_meta': 'This worker computes a meta-data for log files. Output keys: [md5, type_tag, file_type, mime_type, encoding, file_size, filename, import_time]',\n",
        " 'meta': 'This worker computes meta-data. Output keys: [md5, type_tag, file_type, mime_type, encoding, file_size, filename, import_time]',\n",
        " 'meta_deep': 'This worker computes deeper meta-data. Output keys: [md5, sha1, sha256, ssdeep, entropy, file_type, mime_type, encoding, file_size, filename, import_time]',\n",
        " 'pcap_bro': 'This worker runs Bro scripts on a pcap file. Output keys: [log_name:md5...]',\n",
        " 'pcap_meta': 'This worker computes a bunch of meta-data about a pcap file. Output keys: [summary, sessions]',\n",
        " 'pe_classifier': 'This worker classifies PEFiles as Evil or Benign. Output keys: [classification]',\n",
        " 'pe_disass': '(Beta) This worker does disassembly on a sample. Output keys: [decode]',\n",
        " 'pe_features': 'This worker pulls out a bunch of static features from a PEFile. Output keys: [dense_features, sparse_features]',\n",
        " 'pe_indicators': 'This worker uses the static features from the pefile module to look for weird stuff.  Output keys: [indicator_list]',\n",
        " 'pe_peid': 'This worker using the peid database to look for signatures in a PEFile. Output keys: [match_list]',\n",
        " 'strings': 'This worker extracts all the strings from any type of file. Output keys: [string_list]',\n",
        " 'unzip': 'This worker unzips a zipped file. Output keys: [payload_md5s]',\n",
        " 'urls': 'This worker looks for url patterns in strings output. Output keys: [url_list]',\n",
        " 'view': 'This worker generates a view for any type of file. Output keys: [this view calls sub-views, see the concrete class (view_pdf for instance)]',\n",
        " 'view_customer': 'This worker generates a customer usage view. Output keys: [filename, md5, length, customer, upload_date]',\n",
        " 'view_log_meta': 'This worker generates a view for log meta. Output keys: [mime_type, encoding, import_time, file_size]',\n",
        " 'view_meta': 'This worker generates a view for meta about a sample. Output keys: [mime_type, encoding, import_time, ssdeep, entropy, file_size]',\n",
        " 'view_pcap_bro': 'This worker generates a pcap view for the sample. Output keys: [bro_output_log_names...]',\n",
        " 'view_pcap_meta': 'This worker generates a pcap view for the sample. Output keys: [import_time, entropy, file_size, summary, sessions]',\n",
        " 'view_pdf': 'This worker generates a view for a PDF File. Output keys: [filename, filetype, mime_type, encoding, import_time, ssdeep, strings]',\n",
        " 'view_pefile': 'This worker generates a view for a PE File. Output keys: [filename, filetype, mime_type, encoding, import_time, ssdeep, indicators, peid_Matches, classification, disass]',\n",
        " 'view_zip': 'This worker generates a view for a PDF File. Output keys: [filename, filetype, mime_type, payload_md5s, payload_meta]'}"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style=\"float: left; margin: 0px 30px 0px 0px\"><img src=\"files/images/disk.jpg\" width=\"250px\"></div>\n",
      "# Read in the Data\n",
      "<font size=4> Toss a PCAP at workbench, in general there's no constraints on what types of files you can put into workbench. No tools for format XYZ? As a community tool pull requests are encouraged...</font>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# So we can see all the available workers lets load up a file and run some stuff\n",
      "# Okay when we load up a file (or pcap in this case), we get the md5 back\n",
      "filename = '../test_files/pcap_files/rando_web_traffic.pcap'\n",
      "with open(filename,'rb') as f:\n",
      "    md5 = c.store_sample(filename, f.read(), 'pcap')\n",
      "print md5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "bba97e16d7f92240196dc0caef9c457a\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now lets get the meda data for this pcap\n",
      "c.work_request('pcap_meta', md5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "{'pcap_meta': {'encoding': 'binary',\n",
        "  'file_size': 54339570,\n",
        "  'file_type': 'tcpdump capture file (little-endian) - version 2.4 (Ethernet, capture length 65535)',\n",
        "  'filename': '../test_files/pcap_files/rando_web_traffic.pcap',\n",
        "  'import_time': '2014-02-11T14:27:00.073000Z',\n",
        "  'md5': 'bba97e16d7f92240196dc0caef9c457a',\n",
        "  'mime_type': 'application/vnd.tcpdump.pcap',\n",
        "  'type_tag': 'pcap'}}"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now lets run Bro on this pcap\n",
      "results = c.work_request('pcap_bro', md5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Critical Code: Transition from Bro log to Pandas Dataframe\n",
      "\n",
      "# Results is just a dictionary of Bro log file names and their MD5s in workbench\n",
      "# Take the outputs from workbench and toss them into a set of dataframes\n",
      "dataframes = {}\n",
      "for log_name, md5 in results['pcap_bro'].iteritems():\n",
      "\n",
      "    # Every workbench entry contains an md5 field (obviously not interested in that)\n",
      "    if log_name == 'md5': continue\n",
      "        \n",
      "    bro_log_data = c.stream_sample(md5)               # Super magic\n",
      "    dataframes[log_name] = pd.DataFrame(bro_log_data) # Also super magic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style=\"float: left; margin: 0px 30px 0px 0px\"><img src=\"files/images/eyeball_2.jpg\" width=\"250px\"></div>\n",
      "# Lets look at the Data\n",
      "We're going to use some nice functionality in the Pandas dataframe to look at our processed data:\n",
      "\n",
      "    - We can use groupby on the dataframe to see the different header request keys for various agents\n",
      "    - Transform the complicated user-agent string into something more managable (short-agent)\n",
      "    - Generate a 'feature vector' from the header keys"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We're going to pick one of the dataframe to explore in more detail\n",
      "df = dataframes['bro_log_http']\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# List ALL the columns names for this dataframe\n",
      "df.columns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Convert the 'ts' field to an official datetime object\n",
      "df['time'] = pd.to_datetime(df['ts'],unit='s')\n",
      "df['time'].head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Remove columns that don't have any 'meaningful' values in them\n",
      "df.replace('-', np.nan, inplace=True)\n",
      "df.dropna(axis=1, how='all', inplace=True)\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Dataframe has a nice quick descriptive statistics method\n",
      "df.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Describe gives us a sense for the numeric fields for the\n",
      "# categorical fields we can use value counts so we're just\n",
      "# going to loop over all the columns\n",
      "def gist(df):\n",
      "    for column in df.columns:\n",
      "        print '\\n<<< %s >>>' % column\n",
      "        print df[column].value_counts().head()\n",
      "\n",
      "    \n",
      "# The output below is verbose but don't under estimate the power of the 'gist' both\n",
      "# for understanding of the data and quickly uncovering bugs in the reader/data collection.\n",
      "gist(df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Explore pivoting and resampling\n",
      "response_bytes = df[['time','resp_mime_types','response_body_len']]\n",
      "response_bytes['response_body_len'] = response_bytes['response_body_len'].astype(int)\n",
      "print response_bytes.head()\n",
      "pivot = pd.pivot_table(response_bytes, rows='time', values='response_body_len', cols=['resp_mime_types'], aggfunc=sum)\n",
      "sampled_bytes = pivot.resample('10Min', how='sum')\n",
      "sampled_bytes.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "response_bytes.info()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sampled_bytes.info()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let plot it!\n",
      "sampled_bytes.plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Although the plot below looks 'nicer' we have to be SUPER careful with interplolation \n",
      "# of any kind as it can often misrepresent the actual data.\n",
      "interp = sampled_bytes.interpolate(method='linear')\n",
      "interp.plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sampled_bytes_min = pivot.resample('1Min', how='sum')\n",
      "sampled_bytes_min.plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "interp_min = sampled_bytes_min.interpolate(method='linear')\n",
      "interp_min.plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style=\"float: left; margin: 0px 30px 0px 0px\"><img src=\"files/images/processing_data.jpg\" width=\"250px\"></div>\n",
      "# Process the Data\n",
      "We're going to use some nice functionality in the Pandas dataframe to process our data:\n",
      "\n",
      "    - We have both client and server events, just keep the client events for this exercise\n",
      "    - Transform the complicated user-agent string into something more managable (short-agent)\n",
      "    - Generate a 'feature vector' from the header keys"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}